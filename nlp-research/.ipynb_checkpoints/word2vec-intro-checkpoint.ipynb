{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Word Embedding Models\n",
    "\n",
    "**Word2Vec** (Most widely used word embedding models)\n",
    "* https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "**GloVe**\n",
    "* http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "**Difference between Word2Vec and GloVe**\n",
    "word2vec is a \"predictive\" model, whereas GloVe is a \"count-based\" model. Predictive models learn their vectors in order to improve their predictive ability of Loss **(target word|context words; vectors)**. In word2vec, this is cast as a feed-forward neural network and optimized as such using SGD. Count-based models learn their vetors by essentially doing dimensionality reduction on the co-occurrence counts matrix. The counts matrix in the case of GloVe is preprocessed by normalizing the counts and log-smoothing them. This greatly improves the quality of the learned representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GloVe with Gensim Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare a pretrained glove model using gensim word2vec class structure \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# glove_file = \"glove.txt\"\n",
    "tmp_file = \"../../word2vec.txt\"\n",
    "\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523603677749634),\n",
       " ('throne', 0.7664334177970886),\n",
       " ('prince', 0.759214460849762),\n",
       " ('daughter', 0.7473883032798767),\n",
       " ('elizabeth', 0.7460220456123352),\n",
       " ('princess', 0.7424569725990295),\n",
       " ('kingdom', 0.7337411642074585),\n",
       " ('monarch', 0.7214490175247192),\n",
       " ('eldest', 0.7184861898422241),\n",
       " ('widow', 0.7099430561065674)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9288908839225769),\n",
       " ('throne', 0.882325291633606),\n",
       " ('elizabeth', 0.8789501786231995),\n",
       " ('princess', 0.876754879951477),\n",
       " ('daughter', 0.8705160617828369),\n",
       " ('prince', 0.8702554702758789),\n",
       " ('kingdom', 0.8607221841812134),\n",
       " ('eldest', 0.8595449328422546),\n",
       " ('monarch', 0.8584721088409424),\n",
       " ('widow', 0.8549266457557678)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sentence2vec(wv, sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    #just the average of word vectors\n",
    "    return np.mean(wv[tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09763534,  0.50780338, -0.34976998,  0.08669666,  0.51709431,\n",
       "       -0.14514001, -0.16291203, -0.18772399,  0.13293666,  0.10130668,\n",
       "        0.04410134,  0.14747737, -0.46632633, -0.16126433,  0.18636267,\n",
       "        0.39843002,  0.51764631,  0.48091331, -0.81918001, -0.03000333,\n",
       "        0.34736666,  0.40297666, -0.12440801,  0.30112001,  0.27616   ,\n",
       "       -1.21499002, -0.55460668,  0.36502996,  0.12081531,  0.02359533,\n",
       "        2.45146656, -0.11757334,  0.31980333, -0.35405731,  0.07163166,\n",
       "       -0.15091   ,  0.39385998,  0.04269001, -0.15177666, -0.34599003,\n",
       "        0.14388233, -0.05621333, -0.20210667, -0.07296033, -0.03242334,\n",
       "        0.24756564, -0.19698232, -0.33736667,  0.29200733,  0.35319331], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2vec(word_vectors, \"london is awesome\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
