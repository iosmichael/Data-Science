{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Unsupervised Learning Algorithms\n",
    "\n",
    "Unsupervised Learning Algorithms includes:\n",
    "- **Clustering:** discovering hidden groups/subsets in data, e.g. K-means, K-medoids, hierarchical clustering\n",
    "- **Dimensional Reduction:** simplify high dimensional data while preserving information content, e.g. principle component analysis (PCA)\n",
    "- **Imputation:** completing missing values in data, e.g. probabilistic PCA (PPCA)\n",
    "- **Embedding:** converting discrete objects into vector representations, e.g. word2vec\n",
    "- **Density Estimation:** inferring potentially highly complex distributional forms, e.g. mixture of gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Algorithm\n",
    "\n",
    "K-means model for K-clusters defined as follows:\n",
    "* For each $x_n$ there is a one-hot cluster assignment vector\n",
    "$$\\mathbf{r}_n = (r_{n1}, \\ldots, r_{nK})^T \\in \\{0,1\\}^K$$\n",
    " * $r_{nk} = 1$ if $x_n$ assigned to cluster k\n",
    " * $r_{nj} = 0$ if $x_n$ assigned to cluster $k \\neq j$\n",
    "* K-cluster centres, $\\mu_k \\in \\mathcal{R}^D$\n",
    "* requires a distance measure, d, between data-points $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^D$\n",
    "$$d(\\mathbf{x},\\mathbf{y}) = ||\\mathbf{x}-\\mathbf{y}||^2 = (\\mathbf{x}-\\mathbf{y})^T(\\mathbf{x}-\\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_euclidean_distance(datamtx1, datamtx2):\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx1 - NxD array where each row is a vector\n",
    "    datamtx2 - MxD array wehere each row is a vector\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    distances - an NxM matrix whose (i,j)th element is the Euclidean distance\n",
    "        from row i of datamtx1 to jth row of datamtx2\n",
    "    \"\"\"\n",
    "    N, D1 = datamtx1.shape\n",
    "    M, D2 = datamtx2.shape\n",
    "    if D1 != D2:\n",
    "        raise ValueError(\"Incompatible data matrices\")\n",
    "    datamtx1 = datamtx1.reshape(N, D1, 1)\n",
    "    datamtx2 = datamtx2.T.reshape(1, D1, M)\n",
    "    return np.sum((datamtx1-datamtx2)**2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The loss function is**\n",
    "$$\\mathcal{J} = \\sum^N_{n=1}\\sum^K_{k=1}r_{nk}||\\mathbf{x}_n-\\mu_k||^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_kmeans_loss(datamtx, centres, cluster_assignments):\n",
    "    \"\"\"\n",
    "    Evaluates the loss function J for kmeans\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    centres - (KxD) matrix of cluster centres\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    loss - numeric value of the loss function J\n",
    "    \"\"\"\n",
    "    N, D = datamtx.shape\n",
    "    loss = 0.\n",
    "    for cluster_id in np.unique(cluster_assignments):\n",
    "        # identify which points belong to this cluster\n",
    "        assigned_to_this_cluster = (cluster_assignments == cluster_id)\n",
    "        # get the centre as a 1xD matrix\n",
    "        centre = centres[cluster_id,:].reshape((1,D))\n",
    "        clustermtx = datamtx[assigned_to_this_cluster, :]\n",
    "        #\n",
    "        individual_distances = squared_euclidean_distance(clustermtx, centre)\n",
    "        loss += np.sum(individual_distances)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo Code for K-means clustering algorithm\n",
    "* First: choose initial values for each $\\mu_k$\n",
    "* Phase 1: choose $r_{nk}$ to minimise J keeping $\\mu_k$s fixed.\n",
    "* Phase 2: choose $\\mu_k$ to minimise J keeping $r_{nk}$s fixed.\n",
    "* Repeat phases 1 and 2 until convergence\n",
    "\n",
    "Here we are using a technique called **expectation maximization** to accomplish the convergence of this algorithm.\n",
    "\n",
    "* Phase 1 is called the **E step** (Expectation)\n",
    "* Phase 2 is called the **M step** (Maximization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(datamtx, K, initial_centres=None, threshold=0.01, iterations=None):\n",
    "    \"\"\"\n",
    "    Finds K clusters in data using the K-means algorithm\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    K - integer number of clusters to find\n",
    "    initial_centres (optional) - KxD matrix of initial cluster centres\n",
    "    threshold (optional) - the threshold magnitude for termination (for convergence)\n",
    "    iterations (optional) - the maximum number of iterations\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    centres - (KxD) matrix of cluster centres\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "    \"\"\"\n",
    "    if initial_centres is None:\n",
    "        centres = subsample_datapoints(datamtx, K)\n",
    "    else:\n",
    "        centres = initial_centres\n",
    "    # initially change must be larger than threshold for algorithm to run \n",
    "    # at least one iteration\n",
    "    change = 2*threshold\n",
    "    iteration = 1\n",
    "    while change > threshold:\n",
    "        cluster_assignments = kmeans_e_step(datamtx, centres)\n",
    "        centres, change = kmeans_m_step(datamtx, centres, cluster_assignments)\n",
    "        if not iterations is None and iteration >= iterations:\n",
    "            break\n",
    "        iteration += 1\n",
    "    #\n",
    "    return centres, cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The E step**\n",
    "\n",
    "Choose $r_{nk}$ to minimise J keeping $\\mu_k$s fixed:\n",
    "\n",
    "For each n set: (N = number of data points)\n",
    "* $r_{nk} = 1$ if $k = argmin_j||x_n-\\mu_j||^2$\n",
    "* $r_{nk} = 0$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans_e_step(datamtx, centres):\n",
    "    \"\"\"\n",
    "    The E step for the K-means algorithm\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    centres - (KxD) matrix of cluster centres\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "    \"\"\"\n",
    "    N,D = datamtx.shape\n",
    "    K = centres.shape[0]\n",
    "    #datamtx = datamtx.reshape(N,D,1)\n",
    "    #centres = centres.T.reshape(1,D,K)\n",
    "    #distances = np.sum((datamtx - centres)**2, axis=1)\n",
    "    distances = squared_euclidean_distance(datamtx, centres)\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The M step**\n",
    "\n",
    "Choose $\\mu_k$ to minimise J keeping $r_{nk}$s fixed:\n",
    "\n",
    "For each k set: (K = number of clusters)\n",
    "* Taking the following derivatives of the loss function:\n",
    "$$\\nabla_{\\mu_k}\\mathcal{J} = 2\\sum^N_{n=1}r_{nk}(\\mathbf{x}-\\mu_k)$$\n",
    "* Setting to **0** and rearranging gives:\n",
    "$$\\mu_k = \\frac{\\sum_n r_{nk}x_n}{\\sum_n r_{nk}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans_m_step(datamtx, centres, cluster_assignments):\n",
    "    \"\"\"\n",
    "    The M step for the K-means algorithm\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    centres - (KxD) matrix of cluster centres\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    centres - (KxD) matrix of updated cluster centres\n",
    "    change - the magnitude of parameter change (the largest Euclidean distance\n",
    "        between old centres and new centres)\n",
    "    \"\"\"\n",
    "    change = 0.\n",
    "    for k in range(centres.shape[0]):\n",
    "        cluster_points = datamtx[cluster_assignments == k,:]\n",
    "        new_centre = np.mean(cluster_points, axis=0)\n",
    "        old_centre = centres[k,:]\n",
    "        change = max(change, np.sum((new_centre-old_centre)**2))\n",
    "        centres[k,:] = new_centre\n",
    "    return centres, change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-medoids\n",
    "\n",
    "K-means is inappropriate when either data given as a proximity matrix or an average over data-points $x_n$ is not meaningful way to calculate cluster centres, $\\mu_k$. Instead we choose the **data-point closest to all others** in cluster as an alternative centre.\n",
    "\n",
    "**The biggest difference between K-means and K-medoids is that $\\mu_k$ is the average in K-means but a specific data point in K-medoids**\n",
    "\n",
    "Pseudo Code for K-medoids clustering algorithm\n",
    "* Initialize by choosing centres from dataset\n",
    "* E step is the same as K-means\n",
    "* M step chooses:\n",
    "$$\\mu_k = argmin_{x_m \\ s.t.\\ r_{mk} = 1}\\sum_n r_{nk}d(\\mathbf{x}_n, \\mathbf{x}_m)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_kmedoids_loss(proxmtx, medoids, cluster_assignments):\n",
    "    \"\"\"\n",
    "    Evaluates the loss function J for kmeans\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    proxmtx - (NxD) data matrix (array-like)\n",
    "    medoids - (KxD) matrix of cluster centres\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    loss - numeric value of the loss function J\n",
    "    \"\"\"\n",
    "    N = proxmtx.shape[0]\n",
    "    loss = 0.\n",
    "    for cluster_id in np.unique(cluster_assignments):\n",
    "        medoid = medoids[cluster_id]\n",
    "        # identify which points belong to this cluster\n",
    "        cluster_members = (cluster_assignments == cluster_id)\n",
    "        # get the cluster point proximities to this medoid\n",
    "        all_medoid_proximities = proxmtx[:,medoid]\n",
    "        medoid_proximities = all_medoid_proximities[cluster_members]\n",
    "        #\n",
    "        loss += np.sum(medoid_proximities)\n",
    "    return loss\n",
    "\n",
    "def kmedoids(proxmtx, K, initial_medoids=None, threshold=1, iterations=100):\n",
    "    \"\"\"\n",
    "    Finds K clusters in data using the K-medoids algorithm\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    proxmtx - (NxN) proximity matrix (array-like)\n",
    "    K - integer number of clusters to find\n",
    "    initial_medoids (optional) - K vector of initial medoids, , each is a\n",
    "        data-point id 0..(N-1)\n",
    "    threshold (optional) - the threshold magnitude for termination\n",
    "    iterations (optional) - the maximum number of iterations\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    medoids - (K) vector of medoids, each is a data-point id 0..(N-1)\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "    \"\"\"\n",
    "    N = proxmtx.shape[0]\n",
    "    if initial_medoids is None:\n",
    "        medoids = np.random.choice(N, K, replace=False)\n",
    "    else:\n",
    "        medoids = initial_medoids\n",
    "    # initially change must be larger than threshold for algorithm to run \n",
    "    # at least one iteration\n",
    "    change = 2*threshold\n",
    "    total_loss = np.inf\n",
    "    iteration = 1\n",
    "    while change > threshold:\n",
    "        cluster_assignments = kmedoids_e_step(proxmtx, medoids)\n",
    "        medoids, new_total_loss = kmedoids_m_step(\n",
    "            proxmtx, medoids, cluster_assignments)\n",
    "        change = total_loss - new_total_loss\n",
    "        total_loss = new_total_loss\n",
    "        if not iterations is None and iteration >= iterations:\n",
    "            break\n",
    "        iteration += 1\n",
    "    #\n",
    "    return medoids, cluster_assignments\n",
    "\n",
    "def kmedoids_e_step(proxmtx, medoids):\n",
    "    \"\"\"\n",
    "    The E step for the K-means algorithm\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    proxmtx - (NxN) proximity matrix (array-like)\n",
    "    medoids - K vector of medoids, each is a data-point id 0..(N-1)\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "    \"\"\"\n",
    "    # each row is that data-points distance to each of the medoids\n",
    "    distances_to_medoids = proxmtx[:,medoids]\n",
    "    cluster_assignments = np.argmin(distances_to_medoids, axis=1)\n",
    "    return cluster_assignments\n",
    "\n",
    "def kmedoids_m_step(proxmtx, medoids, cluster_assignments):\n",
    "    \"\"\"\n",
    "    The M step for the K-means algorithm\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    centres - (KxD) matrix of cluster centres\n",
    "    cluster_assignments - a vector of N integers 0..(K-1), one per data-point,\n",
    "        assigning that data-point to a cluster\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    centres - (KxD) matrix of updated cluster centres\n",
    "    total_loss - the new total loss of the clustering\n",
    "    \"\"\"\n",
    "    total_loss = 0.\n",
    "    for k, medoid in enumerate(medoids):\n",
    "        # points assigned to this cluster\n",
    "        cluster_points = (cluster_assignments == k)\n",
    "        # the indices where cluster_points is true are the indices of this\n",
    "        # cluster's points\n",
    "        cluster_ids = np.where(cluster_points)[0]\n",
    "        # first filter rows then filter columns to keep only proximities within\n",
    "        # this cluster\n",
    "        cluster_proxmtx = proxmtx[cluster_points, :]\n",
    "        cluster_proxmtx = cluster_proxmtx[:, cluster_points]\n",
    "        # the cluster loss  for each cluster point\n",
    "        cluster_losses = np.sum(cluster_proxmtx, axis=1)\n",
    "        # the new medoid is the id of the point in this cluster with the\n",
    "        # smallest cluster loss\n",
    "        index_of_min_loss = np.argmin(cluster_losses)\n",
    "        cluster_loss = cluster_losses[index_of_min_loss]\n",
    "        new_medoid = cluster_ids[index_of_min_loss]\n",
    "        # add cluster loss to the total\n",
    "        total_loss += cluster_loss\n",
    "        medoids[k] = new_medoid\n",
    "    return medoids, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations of K-means\n",
    "\n",
    "K-means and related algorithms aim to capture sharp distinctions between clusters, but\n",
    "* it may not be clear which cluster a point belongs to\n",
    "* we may want a soft-assignment of every point to every cluster\n",
    "* clusters may overlap significatnly\n",
    "* we want our algorithm to be **robust to noise**\n",
    "* we may also want a measure of our confidence\n",
    "\n",
    "Now we will try a probabilitic approach to tackle the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Gaussians Algorithm\n",
    "\n",
    "Instead of having K-means or K-mediods, we now have K-Gaussians.\n",
    "\n",
    "<img src=\"figures/mog.png\" alt=\"MoG\" style=\"width: 600px;\"/>\n",
    "\n",
    "This image represents the concept of a Mixture of Gaussian distribution. Here we can see that **blue** represents individual separate Gaussians distributions and **red** represent the sum of those distributions. The sum of area under **blue** is equal to the sum of area under **red**, which is equal to 1.\n",
    "\n",
    "$$\\sum^K_{k=1}\\pi_k = 1$$\n",
    "\n",
    "Probability density for **Mixture of *K* Gaussians**:\n",
    "$$p(\\mathbf{x}) = \\sum^K_{k=1}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, \\Sigma_k) = \\sum^K_{k=1}p(k)p(\\mathbf{x}|k)$$\n",
    "\n",
    "*where $\\pi_k = p(k)$, which is the probability of k component in general.*\n",
    "\n",
    "Posterior of component given data-point:\n",
    "$$p(k|\\mathbf{x}) = \\frac{p(k)p(\\mathbf{x}|k)}{\\sum_I p(I)p(\\mathbf{x}|I)}$$\n",
    "\n",
    "*where $p(\\mathbf{x}) = \\sum_I p(I)p(\\mathbf{x}|I)$, which is the probability of data point x occuring in the input space.*\n",
    "\n",
    "**Maximum-likelihood parameters by taking the derivatives with respect to $\\mu_k$s and $\\Sigma_k$s does not lead to a closed form solution**\n",
    "\n",
    "Therefore, we are introducing new **latent** variables:\n",
    "* **z** is K-dimensional one-hot vectors: $z_k \\in \\{0,1\\}$ and $\\sum_k z_k = 1$\n",
    "* Marginals defined by mixing coefficients, e.g. $p(z_k = 1) = \\pi_k$\n",
    "$$p(z) = \\prod^K_{k=1}\\pi_k^{z_k}$$\n",
    "* Conditional for **x** given particular **z** is $p(x|z_k = 1) = \\mathcal{N}(x|\\mu_k, \\Sigma_k)$\n",
    "$$p(x|z) = \\prod^K_{k=1}\\mathcal{N}(x|\\mu_k, \\Sigma_k)^{z_k}$$\n",
    "\n",
    "**Mixture of Gaussians Responsibilities**\n",
    "$$\\begin{align}\n",
    "    \\gamma(z_{nk})& = p(z_{nk}=1|x_n) \\\\\n",
    "                  & = \\frac{p(z_{nk}=1)p(x_n|z_{nk} = 1}{\\sum^K_{I=1}p(z_{nl} = 1)p(x|z_{nl}=1)}\\\\\n",
    "                  & = \\frac{\\pi_k\\mathcal{N}(x_n|\\mu_k, \\Sigma_k)}{\\sum^K_{I=1}\\pi_I\\mathcal{N}(x_n|\\mu_I, \\Sigma_I)}\n",
    "\\end{align}$$\n",
    "* $\\pi_k$ is the prior probability of $z_{nk}=1$\n",
    "* $\\gamma(z_{nk})$ the corresponding posterior probability once we have observed $x_n$\n",
    "* $\\gamma(z_{nk})$ can also be viewed as the **responsibility** $k$th component takes for $x_n$\n",
    "\n",
    "**Expectation-Maximisation for Gaussian Mixtures**\n",
    "\n",
    "Pseudo Code:\n",
    "\n",
    "- Initialize means $\\mu_k$, covariances $\\Sigma_k$ and mixing coefficients $\\pi_k$\n",
    "- **E step**: Evaluate responsibilities $\\gamma(z_{nk})$ using current parameter estimates: $\\mu_k, \\Sigma_k, \\pi_k$\n",
    "- **M step**: Re-estimate parameters $\\mu_k, \\Sigma_k, \\pi_k$ using current responsibilities $\\gamma(z_{nk})$\n",
    "- Repeat E step + M step until convergence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_mog(\n",
    "        datamtx, K, initial_means=None, initial_covmtxs=None,\n",
    "        initial_mixcoefs=None, threshold=1e-10, iterations=None):\n",
    "    \"\"\"\n",
    "    The Expectation maximisation algorithm for mixture of Gaussians\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    K - integer number of components to fit\n",
    "    initial_means (optional) - KxD matrix of initial component means\n",
    "    initial_covmtxs (optional) - KxDxD matrix of initial component covariances\n",
    "    initial_mixcoefs (optional) - K vector of initial mixture coefficients\n",
    "    threshold (optional) - the threshold magnitude for termination\n",
    "    iterations (optional) - the maximum number of iterations\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    means - KxD matrix of component means\n",
    "    covmtxs - KxDxD matrix of component covariances\n",
    "    mixcoefs - K vector of mixture coefficients\n",
    "    responsibilties - NxK matrix of soft-assignments, one row per datapoint, one\n",
    "        column per component\n",
    "    \"\"\"\n",
    "    N, D = datamtx.shape\n",
    "    # initialise the cluster parameters\n",
    "    if initial_means is None:\n",
    "        # default means are sampled from the data\n",
    "        means = subsample_datapoints(datamtx, K)\n",
    "    else:\n",
    "        means = initial_means\n",
    "        assert means.shape == (K,D), \"Initial means are incompatible\"\n",
    "    if initial_covmtxs is None:\n",
    "        # default covariances are the identity matrix\n",
    "        covmtxs = np.empty((K,D,D))\n",
    "        for k in range(K):\n",
    "            covmtxs[k,:,:] = np.identity(D)\n",
    "    else:\n",
    "        covmtxs = initial_covmtxs\n",
    "        assert covmtxs.shape == (K,D,D), \"Initial covariances are incompatible\"\n",
    "    if initial_mixcoefs is None:\n",
    "        # default mixture coefficients is all equal\n",
    "        mixcoefs = np.ones(K)/K\n",
    "    else:\n",
    "        covmtxs = initial_covmtxs\n",
    "        assert covmtxs.shape == (K,D,D), \"Initial covariances are incompatible\"\n",
    "    # initially change must be larger than threshold for algorithm to run \n",
    "    # at least one iteration\n",
    "    change = 2*threshold\n",
    "    iteration = 1\n",
    "    while change > threshold:\n",
    "        log_resps = em_e_step(datamtx, means, covmtxs, mixcoefs)\n",
    "        old_means = means\n",
    "        old_covmtxs = covmtxs\n",
    "        old_mixcoefs = mixcoefs\n",
    "        means, covmtxs, mixcoefs = em_m_step(datamtx, log_resps)\n",
    "        # the change is the sum of squared differences across all parameters \n",
    "        change = np.sum((means-old_means)**2) \\\n",
    "            + np.sum((covmtxs-old_covmtxs)**2) \\\n",
    "            + np.sum((mixcoefs-old_mixcoefs)**2)\n",
    "        if not iterations is None and iteration >= iterations:\n",
    "            break\n",
    "        iteration += 1\n",
    "    #\n",
    "    return means, covmtxs, mixcoefs, log_resps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**E step**: Evaluate responsiblities for all n and k\n",
    "$$\\gamma(z_{nk}) = \\frac{\\pi_k\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum^K_{I=1}\\pi_I\\mathcal{N}(x_n|\\mu_I,\\Sigma_I)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_e_step(datamtx, means, covmtxs, mixcoefs):\n",
    "    \"\"\"\n",
    "    The E step for the EM algorithm -- mixture of Gaussians\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    means - (KxD) matrix of component mean estimates\n",
    "    covmtxs - (KxDxD) matrix of component covariance estimates\n",
    "    mixcoefs - K vector of mixture coefficient estimates\n",
    "  \n",
    "    returns\n",
    "    -------\n",
    "    log_resps - an NxK matrix of logs of responsibility vectors, namely\n",
    "        logs of soft-assignments of each data-point (row) to each component\n",
    "    \"\"\"\n",
    "    N,D = datamtx.shape\n",
    "    K = means.shape[0]\n",
    "    # initially we construct the log_resps in unnormalised way then\n",
    "    # later we will normalise by ensuring each row sums to 1\n",
    "    log_resps = np.empty((N,K))\n",
    "    for k in range(K):\n",
    "        mean = means[k,:]\n",
    "        covmtx = covmtxs[k,:,:]\n",
    "        mixcoef = mixcoefs[k]\n",
    "        # calculate N(x|mu_k, Sigma_k) for each x\n",
    "        kth_component_logdensities = multivariate_normal.logpdf(datamtx, mean, covmtx)\n",
    "        # now insert pi_k *N(x|mu_k, Sigma_k) as kth column of matrix\n",
    "        log_resps[:,k] = np.log(mixcoef) + kth_component_logdensities\n",
    "    # now renormalise the rows (so exponential of each row sums to 1)\n",
    "    # We work in logs because they are less sensitive to precision errors\n",
    "    normaliser = np.log(np.exp(log_resps).sum(axis=1))\n",
    "    log_resps = log_resps - normaliser.reshape((N,1))\n",
    "    return log_resps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**M step**: Re-estimate parameters:\n",
    "$$\\mu_k^{new} = \\frac{1}{N_k}\\sum^N_{n=1}\\gamma(z_{nk})x_n$$\n",
    "$$\\Sigma_k^{new} = \\frac{1}{N_k}\\sum^N_{n=1}\\gamma(z_{nk})(x_n-\\mu_k^{new})^T(x_n-\\mu_k^{new})$$\n",
    "$$\\pi_k^{new} = \\frac{N_k}{N}$$\n",
    "**where $N_k = \\sum_{n=1}^N\\gamma(z_{nk})$**, $N_k$ can be thought of as the total number of data-points that component k takes responsibility for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_m_step(datamtx, log_resps):\n",
    "    \"\"\"\n",
    "    The M step for the EM algorithm -- mixture of Gaussians\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    log_resps - an NxK matrix of logs of responsibility vectors, i.e. logs of\n",
    "        soft-assignments of each data-point (row) to each component\n",
    "  \n",
    "    returns\n",
    "    -------\n",
    "    means - (KxD) matrix of component new mean estimates\n",
    "    covmtxs - (KxDxD) matrix of component new covariance estimates\n",
    "    mixcoefs - K vector of new mixture coefficient estimates\n",
    "    \"\"\"\n",
    "    N, D = datamtx.shape\n",
    "    K = log_resps.shape[1]\n",
    "    # calculate the estimated number of points assigned to each component\n",
    "    Nks = np.exp(log_resps).sum(axis=0)\n",
    "    mixcoefs = Nks/N\n",
    "    means = np.empty((K,D))\n",
    "    covmtxs = np.empty((K, D, D))\n",
    "    for k, Nk in enumerate(Nks):\n",
    "        # resps_k - shorthand for the responsibilities rnk) for all n\n",
    "        resps_k = np.exp(log_resps[:,k])\n",
    "        meank = np.sum(datamtx*resps_k.reshape((N,1)), axis=0)/Nk\n",
    "        means[k,:] = meank\n",
    "        # construct a sequence of vectors (x_n - mu_k) as matrix objects\n",
    "        diffs_k = ( np.matrix(xn - meank).reshape((D,1)) for xn in datamtx )\n",
    "        # a sequence of component matrices each a term from sum of Result (7.6)\n",
    "        covmtx_components = (\n",
    "            rnk*diff*diff.T for rnk, diff in zip(resps_k, diffs_k))\n",
    "        # performs sum from Result (7.6) giving covariance for component k\n",
    "        covmtxs[k, :, :] = np.sum(covmtx_components, axis=0)/Nks[k]\n",
    "    return means, covmtxs, mixcoefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the test data\n",
    "def uniform_data(N):\n",
    "    data = np.random.uniform(-10,10,(N,2))\n",
    "    return data\n",
    "\n",
    "def sample_mog(N, means, covmtxs, mixcoefs):\n",
    "    \"\"\"\n",
    "    Sample 2d data from a mixture of gaussians distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N - the number of data points to output\n",
    "    means - a KxD array of mean vectors (one per row)\n",
    "    covmtxs - a KxDxD array of covariance matrices each row slice covmtxs[k,:,:]\n",
    "        is a DxD covariance matrix \n",
    "    mixcoefs - a 1d array (length K) of mixture coefficients (must sum to 1)\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datamtx - randomly sampled data NxD array\n",
    "    latent_components - a vector of length N, specifying which component \n",
    "        generated the data (as an int)\n",
    "    \"\"\"\n",
    "    if not math.isclose(np.sum(mixcoefs), 1.):\n",
    "        # checks if the sum of the mixture coefficients is very close to 1.\n",
    "        # it should be (precision errors sometimes mean we do not get exactly 1)\n",
    "        raise ValueError(\"Mixture coefficients do not sum to  1\")\n",
    "    K, D = means.shape\n",
    "    # choose which component each sample comes from\n",
    "    latent_components = np.random.choice(\n",
    "        np.arange(K), N, replace=True, p=mixcoefs)\n",
    "    datamtx = np.empty((N,D))\n",
    "    for k in range(K):\n",
    "        mean = means[k,:]\n",
    "        covmtx = covmtxs[k,:,:]\n",
    "        # identify which points are formed from this component\n",
    "        component_points = (latent_components == k)\n",
    "        Nk = np.sum(component_points)\n",
    "        # generate points from this component\n",
    "        datamtx[component_points] = \\\n",
    "            np.random.multivariate_normal(mean, covmtx, size=Nk)\n",
    "    # return data matrix and latent components\n",
    "    return datamtx, latent_components\n",
    "\n",
    "#sample data using mixture of gaussians technique\n",
    "def sample_data(N):\n",
    "    #means as three clusters and (x,y) coordinate\n",
    "    means = np.empty((3,2));\n",
    "    means[0,:] = [-5, -1]\n",
    "    means[1,:] = [0, 1]\n",
    "    means[2,:] = [5, -1]\n",
    "    # covariance matrices as three identity matrix\n",
    "    covmtxs = np.empty((3,2,2))\n",
    "    covmtxs[0,:,:] = np.identity(2)\n",
    "    covmtxs[1,:,:] = np.identity(2)\n",
    "    covmtxs[2,:,:] = np.identity(2)\n",
    "    # and the mixture coefficients\n",
    "    \n",
    "    mixcoefs = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "    datamtx, latent_components = sample_mog(\n",
    "        N, means, covmtxs, mixcoefs)\n",
    "    return datamtx, latent_components, means, covmtxs, mixcoefs\n",
    "\n",
    "def subsample_datapoints(datamtx, K):\n",
    "    \"\"\"\n",
    "    Subsamples K data-points from a matrix\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    datamtx - (NxD) data matrix (array-like)\n",
    "    K - integer number of points to sample\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    subsamples - a random sample of K data-points\n",
    "    \"\"\"\n",
    "    ids = np.arange(datamtx.shape[0])\n",
    "    np.random.shuffle(ids)\n",
    "    cluster_ids = ids[:K]\n",
    "    subsamples = datamtx[cluster_ids,:]\n",
    "    return subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means: [[-4.02210279 -1.41402524]\n",
      " [-5.30305938 -0.83713822]\n",
      " [ 1.77590679  0.32114677]]\n",
      "true means: [[-5. -1.]\n",
      " [ 0.  1.]\n",
      " [ 5. -1.]]\n",
      "covmtxs: [[[ 0.2956924   0.52215939]\n",
      "  [ 0.52215939  1.15392841]]\n",
      "\n",
      " [[ 0.62050463  0.14329024]\n",
      "  [ 0.14329024  0.51176778]]\n",
      "\n",
      " [[ 6.78889945 -2.14533983]\n",
      "  [-2.14533983  2.11556711]]]\n",
      "true covmtxs: [[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "K = 3\n",
    "datamtx, latent_components, true_means, true_covmtxs, mixcoefs = sample_data(N)\n",
    "means, covmtxs, mixcoefs, log_resps = em_mog(datamtx, K)\n",
    "print(\"means:\",means)\n",
    "print(\"true means:\", true_means)\n",
    "print(\"covmtxs:\",covmtxs)\n",
    "print(\"true covmtxs:\", true_covmtxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
